{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f183433-c972-40a4-8b26-c1c1cb4a39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/bart-large-cnn\n",
      "Model loaded successfully. Using device: cpu\n",
      "Model saved successfully as text_summarizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import gradio as gr\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class TextSummarizer:\n",
    "    def __init__(self, model_name=\"facebook/bart-large-cnn\"):\n",
    "        \"\"\"\n",
    "        Initialize the summarizer with a specific BART model\n",
    "        Args:\n",
    "            model_name: The name of the model to use (default: facebook/bart-large-cnn)\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model loaded successfully. Using device: {self.device}\")\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count the number of tokens in the text\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def chunk_text(self, text, max_chunk_size=1024):\n",
    "        \"\"\"\n",
    "        Split text into chunks that fit within the model's token limit\n",
    "        Args:\n",
    "            text: The text to chunk\n",
    "            max_chunk_size: Maximum number of tokens per chunk\n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_chunk_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If this sentence alone exceeds limit, split it (simplified approach)\n",
    "            if sentence_tokens > max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    current_chunk = []\n",
    "                    current_chunk_size = 0\n",
    "                \n",
    "                # Add a truncated version of the long sentence\n",
    "                trunc_sentence = self.tokenizer.decode(\n",
    "                    self.tokenizer.encode(sentence, max_length=max_chunk_size, truncation=True)[:-2]\n",
    "                )\n",
    "                chunks.append(trunc_sentence)\n",
    "                continue\n",
    "            \n",
    "            # Check if adding this sentence would exceed the limit\n",
    "            if current_chunk_size + sentence_tokens > max_chunk_size:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_chunk_size = sentence_tokens\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_chunk_size += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def summarize(self, text, max_length=150, min_length=40, length_penalty=2.0, \n",
    "                  num_beams=4, early_stopping=True):\n",
    "        \"\"\"\n",
    "        Summarize the given text\n",
    "        Args:\n",
    "            text: The text to summarize\n",
    "            max_length: Maximum length of the summary\n",
    "            min_length: Minimum length of the summary\n",
    "            length_penalty: Length penalty for beam search\n",
    "            num_beams: Number of beams for beam search\n",
    "            early_stopping: Whether to stop when num_beams complete sentences are found\n",
    "        Returns:\n",
    "            Summary text\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if text is too long and needs chunking\n",
    "        if self.count_tokens(text) > 1024:\n",
    "            return self.summarize_long_text(text, max_length, min_length, length_penalty, num_beams, early_stopping)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = self.tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs, \n",
    "            max_length=max_length, \n",
    "            min_length=min_length, \n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams, \n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        return summary, processing_time\n",
    "\n",
    "# Save the model using pickle\n",
    "def save_model(summarizer, filename=\"text_summarizer.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(summarizer, f)\n",
    "\n",
    "# Load the model using pickle\n",
    "def load_model(filename=\"text_summarizer.pkl\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Initialize the summarizer and save it\n",
    "summarizer = TextSummarizer()\n",
    "save_model(summarizer)\n",
    "\n",
    "print(\"Model saved successfully as text_summarizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36ffa9-04f6-427a-84fb-dbb8832e39f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
